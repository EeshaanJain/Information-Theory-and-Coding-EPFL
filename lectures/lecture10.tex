\chapter{Lecture 10}
\section{Data Transmission}
\begin{figure}[H]
\centering
\begin{tikzpicture}[thick,
    gain/.style = {
        draw, 
        ellipse,
        minimum height = 2em,
        outer sep=0},
    TFblock/.style= {
        draw,  
        minimum size=1cm},
        node distance=1.6cm]
 \tikzstyle{every node}=[font=\scriptsize]
\node [gain,fill=cyan!20] (a) {Source};
 \node [TFblock,fill=green!20,] (b) [right= of a] {Encoder};
 \node [gain,fill=orange!20,text width=3cm,align=center] (c) [right =of b] {Communication Medium};
\node [TFblock,fill=orange!20] (d)[right =of c] {Decoder};
\node [draw] (e)[right =of d] {$v_1, \dots, v_k$};
 \draw[-latex] 
    (a.east) edge node[above] {$u_1, \dots, u_k$}(b.west)
    (b.east) edge node[above] {$x_1, \dots, x_n$}(c.west)
    (c.east) edge node[above] {$y_1, \dots, y_n$}(d.west)
    (d.east) -- node[above] {} ++(1.6,0) ;
 
% Labels
\node[below=0.8cm] at (c){Noisy channel};
%\node[below=0.8cm] at (barycentric cs:c=1,d=1){ Phase compensation block};
 \node[below=0.8cm] at (e){Estimates of $u_1, \dots, u_k$};
\end{tikzpicture}
\end{figure}
The efficiency of the transmission (data symbols/channel-wise) is defined as $k/n$, and the reliability of the system (symbol-error rate) is given as 
\[\text{Reliability} = \frac{1}{k} \sum_{i=1}^k \Pr(u_i \neq v_i) = \EE\bigg[\frac{\sum_{i=1}^k \mathbb{1}\{u_i \neq v_i\}}{k}\bigg]\]
\begin{eg}
Consider that a channel accepts a binary input $X_i \in \{0,1\}$ and produces a binary output $Y_i \in \{0, 1\}$ (independent for $i=1, 2, \dots$) as
\[Y_i = \begin{cases}
X_i & 1-p = 0.9 \\
\overline{X_i} & p = 0.1
\end{cases}
\]
Such a channel is called the binary symmetric channel. If we try to send a data bit $k=1$, $u_1 = \{0, 1\}$ and define the encoders as
\begin{itemize}
    \item Encoder 1: $u_1\to X_1 = u_1$, efficiency = 1 and $\Pr(\text{error}) = p = 0.1$
    \item Encoder 2: $u_1 \to X_1X_2 = u_1u_1$, efficiency = $1/2$ but $\Pr(\text{error}) = p = 0.1$.
    \item Encoder 3: $u_1 \to X_1X_2X_3 = u_1u_1u_1$, efficiency = $1/3$ but now, we have (using majority rule decoding)
    \[\Pr(\text{error}) = p^3 + p^2(1-p) + p^2(1-p) + p^2(1-p) = 3p^2-2p^3\]
\end{itemize}
Thus we can write that Encoder$_{2n+1}$ has efficiency $1/(2n+1) \to 0 as n\uparrow$ and 
\[\Pr(\text{error}) = \sum_{i=n+1}^{2n+1} \binom{2n+1}{i} p^i (1-p)^{2n+1-i} \to 0 \text{ as } n\uparrow\]
Thus, we increase the reliability of the system but decrease its efficiency
\end{eg}
\section{Channels}
To describe a channel and a scheme to use it, we need to specify:
\begin{enumerate}
    \item Data Transmission Scheme: $p(X_1), p(X_2|X_1Y_1), \dots p(X_i|X^{i-1}Y^{i-1})$
    \item Channel Specification: $p(Y_1|X_1), p(Y_2|X_2X_1Y_1), \dots p(Y_i|X^{i-1} Y^{i-1} X_i)$
\end{enumerate}
\begin{definition}[Memorylessness]
We say that the channel is memoryless if $\Pr(Y_i|X^{i-1}Y^{i-1}) = \Pr(Y_i|X_i)$
\end{definition}
\begin{definition}[Feedbackless]
We say that a transmission scheme is \textit{without feedback} if \\$p(X_i|X^{i-1}Y^{i-1}) = p(X_i|X^{i-1})$
\end{definition}
\begin{theorem}
If we have a memoryless channel and transmission without feedback, then
\begin{align*}&\Pr(X^n = x^n, Y^n = y^n) = \Pr(X^n=x^n) \prod_{i=1}^n p(y_i|x_i) \\
\equiv &\Pr(Y^n = y^n) = \prod_{i=1}^n \Pr(Y_i=y_i|X_i=x_i)
\end{align*}
\end{theorem}
\begin{proof}
\begin{align*}
    \Pr(X^n = x^n, Y^n = y^n) &= \prod_{i=1}^n p(x_i|x^{i-1}y^{i-1}) p(y_i|x^{i-1}y^{i-1}x_i) \\
    &= \bigg(\prod_{i=1}^n p(x_i|x^{i-1})\bigg) \bigg(\prod_{i=1}^n p(y_i|x_i)\bigg) \\
    &= \Pr(X^n = x^n)\prod_{i=1}^n p(y_i|x_i)
\end{align*}
\end{proof}
\begin{corollary}
For a memoryless channel with no-feedback transmission,
\[H(Y^n|X^n) = \sum_{i=1}^n H(Y_i|X_i)\]
\end{corollary}
\begin{proof}
    Take logarithm and expectation in the proof above.
\end{proof}
\begin{definition}[Stationarity]
A memoryless channel is said to be stationary if
\[\Pr(Y_i=y_i|X_i=x) = \Pr(Y_1=y|X_1=x) \;\forall\;i,x,y\]
\end{definition}
\begin{definition}[Capacity]
For a memoryless, stationary channel $P(Y|X)$, we can define the \textit{Capacity} as 
\[C(P_{Y|X}) = \max_{p(X)} I(X;Y)\]
\end{definition}
\begin{theorem}
For a memoryless stationary channel $P_{Y|X}$ and transmission without feedback
\[I(X^n;Y^n) \leq \sum_{i=1}^n I(X_i;Y_i) \leq n C(P_{Y|X})\]
\end{theorem}
\begin{proof}
The second inequality is trivial, since we are just summing the mutual information $n$ times. For the first inequality,
\begin{align*}
    I(X^n;Y^n) &= H(Y^n) - H(Y^n|X^n) = H(Y^n) - \sum_{i=1}^n H(Y_i|X_i) \\
    &\leq \sum_{i=1}^n H(Y_i) - \sum_{i=1}^n H(Y_i|X_i) \\
    &= \sum_{i=1}^n I(Y_i;X_i)
\end{align*}
\end{proof}
\begin{corollary}
    For data transmission  with a stationary memoryless channel $P_{Y|X}$ without feedback, we have
    \[I(U^k; V^k) \leq n C(P_{Y|X})\]
\end{corollary}
\begin{proof}
    The transmission claims that $U^k - X^n - Y^n - V^k$ is a Markov chain. Data processing inequality gives $I(U^k; V^k) \leq I(X^n; Y^n) \leq nC(P_{Y|X})$
\end{proof}
\begin{theorem}
Suppose $U_1, U_2, \dots$ is a stationary source with entropy rate $\Hcal = \lim_{k\to\infty} \frac{1}{k} H(U^k)$ and the channel is stationary memoryless. Then
\[\frac{1}{k} H(U^k|V^k) \geq \Hcal - \frac{n}{k} C(P_{Y|X})\]
\end{theorem}
\begin{proof}
    \begin{align*}
        \frac{1}{k} H(U^k|V^k) &= \frac{1}{k} H(U^k) - \frac{1}{k} I(U^k;V^k)\\
        &\geq \Hcal - \frac{1}{k} I(U^k;V^k) \quad \text{ (Since $\Hcal$ is a lower bound)} \\
        &\geq \Hcal - \frac{n}{k} C(P_{Y|X})
    \end{align*}
\end{proof}
\begin{lemma}[Multi-Letter Fano's Inequality]
Recall Fano's inequality: for $U, V \in Ucal$, 
\[H(U|V) \leq h_2(p) + p\log(|\Ucal| - 1) \quad \text{with }p=\Pr(U\neq V)\]
Suppose $U_1, \dots, U_k, V_1, \dots, V_k \in \Ucal$ and let $p_i = \Pr(U_i \neq V_i)$ and $p = \frac1k \sum_{i=1}^k p_i$, then
\[\frac{1}{k} H(U^k|V^k) \leq h_2(p) + p\log(|\Ucal| - 1)\]
\end{lemma}
\begin{proof}
\begin{align*}
    \frac{1}k H(U^k|V^k) &= \frac{1}{k} \sum_{i=1}^k H(U_i|U^{i-1} V^k) 
    \leq \frac{1}{k} \sum_{i=1}^k H(U_i|V_i)
\end{align*}
Now, using Fano's inequality
\begin{align*}
    \frac{1}k H(U^k|V^k) &\leq \frac{1}{k} \sum_{i=1}^k\Big[ h_2(p_i) + p_i\log(|\Ucal|-1)\Big]\\
    &= \frac1k \sum_{i=1}^k h_2(p_i) + p\log(|\Ucal|-1)
\end{align*}
We claim that $\frac1k \sum_{i=1}^k h_2(p_i) \leq h_2\bigg(\frac1k \sum_{i=1}^k p_i\bigg)$. To prove this, consider $(w,z)$ with $w \in \{1, \dots, k\}$ uniformly and $z \in \{0,1\}$ with $\Pr(z=1|w=i) = p_i$.
\begin{align*}
    H(z|w) &= \sum_{i=1}^k \Pr(w=i)H(z|w=i) = \frac{1}{k} \sum_{i=1}^k h_2(p_i) \\
    H(z) &= h_2(\Pr(z=1)) = h_2\bigg(\frac1k \sum_{i=1}^k p_i\bigg) = h_2(p)
\end{align*}
\end{proof}
\begin{theorem}
For any enc, dec with the data transmission scheme as shown in the figure above and $V_i \in \Ucal$, $U_1, U_2, \dots$ stationary with entropy rate $\Hcal$, $p = \frac1k \sum_{i=1}^k \Pr(U_i \neq V_i)$,
\[h_2(p) + p\log(|\Ucal| - 1) \geq \Hcal - \bigg(\frac kn\bigg)^{-1} C(P_{Y|X})\]
Note that the LHS is small if $p$ is small, and it doesn't depend on $k$ while the RHS is large if $(k/n)$ is large.
\end{theorem}
\begin{remark}
\[\Hcal \frac{k}{n} \leq \frac{\Hcal C(P_{Y|X})}{\Hcal - [h_2(p) + p\log(|\Ucal| - 1)]} \approx C(P_{Y|X})\]
If $\Hcal \frac kn > C(P_{Y|X})$, then there exists $p_0 > 0$ such that any encoder or decoder will suffer from symbol error rate $> p_0$.
\end{remark}

